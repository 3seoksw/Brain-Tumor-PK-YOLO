\documentclass{article} % For LaTeX2e
\usepackage[preprint]{stylefile}
\usepackage{stylefile,times}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{array, makecell, amsmath}
\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Brain Tumor Detection}

\author{
  Damilola Olaiya\footnotemark[1] \\
  Carleton University \\
  Ottawa, Canada \\
  \texttt{damilolaolaiya@cmail.carleton.ca}
  \And
  Hamza Cecen\footnotemark[1] \\
  Carleton University \\
  Ottawa, Canada \\
  \texttt{hamzacecen@cmail.carleton.ca}
  \And
  WooSeok Kim\footnotemark[1] \\
  Carleton University \\
  Ottawa, Canada \\
  \texttt{wooseokkim@cmail.carleton.ca} \\ \\
  \textsuperscript{*}These authors contributed equally to this work.
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}


\maketitle

\begin{abstract}
Magnetic Resonance Imaging (MRI) has a central role in the detection and clinical assessment of brain tumors.
Recent advancements in machine learning and object detection, especially YOLO-related frameworks, have significantly improved automated tumor localization in 2D multiplanar MRI slices.
One of these, Pretrained Knowledge Guided YOLO (PK-YOLO) has demonstrated state-of-the-art performance by integrating SparK-pretrained RepViT backbone and specialized optimization techniques.
However, the original PK-YOLO design exhibits notable limitations, primarily stemming from a plane-shift domain gap, with its backbone is pretrained exclusively on axial images, resulting in substantial performance disparities across planes.

To solve this limitation, we propose a 3D Multiplanar Fusion framework.
In this method, three different backbones, each of which is pretrained on an anatomical plane (\textit{i.e.}, axial, coronal, or sagittal), are used.
Then the outputs of the backbones are combined using an adaptive router module that learns how much weight to give each plane.
%%%%%% Summary of the results %%%%%%
%This strategy helps the model understand features from all three planes more effectively, reduces the gap between different MRI views, and improves overall accuracy.
%As a result, the model becomes more reliable and better suited for detecting brain tumors across multiplanar MRI scans.

\end{abstract}

\section{Introduction}
Over the past few years, advances in machine learning have opened up the possibilities to integrate deep learning methods into the medical domain\cite{asif2025advancements}.
Throughout active research, several papers employ multiple approaches (\textit{e.g.}, image processing, convolutional networks, attention-based networks) to improve brain tumor detection performance \cite{rasool2025brain}.

Spotting brain tumors matters a lot when looking at medical scans, since finding them early and correctly helps doctors figure out what’s wrong, decide on treatments, and keep track of how patients are doing \cite{braintumerbasic}. 
MRIs are common for this job because they show fine details and distinguish soft tissues well \cite{mrisurvey}. 
Especially useful are multi-angle MRI views, like top, front, and side cuts, which give different body viewpoints, making it easier for physicians to pin down where the tumor sits and understand its shape.

Alongside recent computer vision technologies, YOLO-based models have drawn significant attention due to their efficiency, robust detection performance, and real-time capabilities. However, applying YOLO architectures in medical imaging remains challenging because MRI scans differ substantially from natural images in terms of intensity patterns, noise characteristics, and underlying anatomical structures.

To address these challenges, M.~Kang \textit{et al.}~\cite{kang2025pk} proposed a new You Only Look Once (YOLO)-based \cite{redmon2016lookonceunifiedrealtime} brain tumor detection model by leveraging 2D multiplanar Magnetic Resonance Imaging (MRI) slices for model training and fine-tuning.
To tackle these issues, Kang’s team introduced PK-YOLO - a fresh take on YOLO built just to detect brain tumors through 2D MRI views from different angles.


\section{Limitations of the Previous Work}\label{sec:limits}
While PK-YOLO shows state-of-the-art performance, the performance across three different planes exhibits its limitation.
%it has limitations because of how multiplanar MRI scans are built.
This model is trained only on horizontal slices, although brain imaging naturally covers three slices such as horizontal (axial), front-to-back (coronal), and side-to-side (sagittal).
Since every angle or view shows different information, using only one slice can lead to gaps when processing others.

This becomes more obvious when looking at the experimental results from the paper.
As shown in Table.~\ref{tab:perf_comp}, PK-YOLO performs well in axial views, which is expected because the backbone is already pretrained in axial views.
However, the performance drop can be found when switching planes from axial views to coronal or sagittal views.
For instance, PK-YOLO's mAP$_{50}$ scored 0.947 in axial dataset, but its metric dropped to 0.805 and 0.582 in coronal and sagittal planes dataset, respectively.

%PK-YOLO performs strongly on axial views (mAP = 0.947), which is expected because the backbone is already pretrained on axial views.
% However, accuracy immediately reduces when switching planes like performance drops to (mAP = 0.793) on coronal and then falls further (mAP = 0.582) on sagittal as well. When comparing these values side-by-side, it becomes clear that PK-YOLO loses more than one-third of its detection accuracy when moving from axial to sagittal. This large gap reveals a serious generalization issue.

This performance gap between planes is due to how PK-YOLO relies on one backbone pretrained in axial views for every plane.
%A key point from the study is how PK-YOLO relies on one backbone for every plane.
Since this backbone is optimized only for axial views, it ends up using identical features even when dealing with images that look quite different.
Without a way to adapt to characteristics of each plane, performance of the model changes depending on which MRI view is used. 

Even though PK-YOLO outperforms other models such as RT-DETR or LW-DETR in the axial plane, it still struggles when plane-shift occurs on MRI angles.
%Even though PK-YOLO outperforms other models such as RT-DETR or LW-DETR on the axial plane, it still struggles when dealing with varied MRI angles.
% Because of this, an improved method is needed that can better adapt to each plane and reduce the performance gap observed across axial, coronal, and sagittal slices. (I don't think we need this sentence)

\begin{table}
    \centering
    \begin{tabular}{c|cccc}
        \Xhline{2\arrayrulewidth}
        Dataset & Precision & Recall & mAP$_{50}$ & mAP$_{50-95}$ \\
        \hline
        axial & 0.858 & 0.896 & 0.947 & 0.681 \\
        \hline
        coronal & 0.834 & 0.793 & 0.805 & 0.689 \\
        \hline
        sagittal & 0.476 & 0.845 & 0.582 & 0.382 \\
        \Xhline{2\arrayrulewidth}
    \end{tabular}
    \newline
    \caption{PK-YOLO's performance comparison table across different models and MRI planes.}
    \label{tab:perf_comp}
\end{table}

%\begin{figure}[h!]
%    \centering
%    \includegraphics[width=0.6\linewidth]{ProjReportTemplate/figures/table_performance.png}
%    \caption{Performance comparison table across different models and MRI planes}
%    \label{fig:table_performance}
%\end{figure}


\section{Related Work}
Research on brain tumor detection using deep learning largely intersects with three key areas: DETR framework, brain tumor detection, and multiplanar MRI analysis. The PK-YOLO paper introduces contributions in all three areas, and the prior work they reference can be grouped into the following themes.

\subsection{DETR Framework}
Object detection methods using the DETR (DEtection TRansformer) \cite{detrframeworkbasic} system have improved much better recently, mostly thanks to smarter training strategies and backbone pretraining.
The first versions, like UP-DETR \cite{updetr}, introduced unsupervised pretraining which showed that self-taught features can help boost accuracy later on.
Later works, such as Group DETR v2 \cite{groupdetr} or Co-DETR \cite{codetrs}, advanced the DETR performance ahead by exploring how both encoder and decoder are pretrained while mixing up label matching ways, scoring high on benchmarks such as MS COCO.

Along with these developments, lighter and faster DETR versions have appeared.
Instead of relying on pretraining, RT-DETR \cite{rtdetr} uses decoder layers to offer speed options on demand.
LW-DETR \cite{lwdetr} increases efficiency using special pretraining strategies made just for it.
By adding salience-guided supervision during learning, Salience DETR \cite{salience} becomes stable like speeding up training while keeping the results reliable.
Despite these advances, according to the PK-YOLO study, models based on DETR aren't used much yet for detecting brain tumors in MRI scans.
Moreover, current DETR still struggles to match YOLO-based systems in the medical domain because they need heavy computing power and adapt poorly to new domains.

\subsection{Brain Tumor Detection}
YOLO-based models pop up a lot in regular image detection, but they don't show up much in medical imaging, especially when it comes to finding brain tumors.
Lately, researchers tried to adapt them for hospital use.
For example, RCS-YOLO \cite{rcsyolo} uses focused area blocks that help pin down where tumors sit.
On the other hand, BGF-YOLO \cite{bgfyolo} upgrades YOLOv8 by blending multi-level detail filters to sharpen tiny tumor clues.
Finally, different versions like YOLOv5 \cite{yolov5}, YOLOv8, or YOLOv9 \cite{yolov9} got tested on brain MRIs too.
Still, problems stick around, especially with spotting smaller growths and working well across varied scan angles like axial, coronal and sagittal.


\subsection{Multiplanar MRI Analysis}
Multiplanar MRI scans use axial, coronal and sagittal slices to detect tumors that might look different depending on angle.
Prior research has looked into ways to combine these angles effectively.
Instead of one method, Piantadosi \cite{piantadosi} used groups of 2D networks to better separate MRI tissue types in brain images.
Similarly, the MPS-FFA model \cite{mpsffa} introduces mixed features from various scales and planes to help sort Alzheimer’s cases, showing how blending directional data boosts accuracy.
Some research into detecting objects like Barbato and Menga’s use of YOLOv5m, found low average accuracy when checking multi-view MRI images, showing it's difficult to detect lesions from different body angles.
In those works, common issues pop up now and then: lesions change size or shift place depending on the view, thinner image layers lead to more tiny lesions appearing, and also training one system to handle all three imaging directions remains difficult.

% \begin{table*}
%   \centering
%   \label{table:detr_summary}
%   \renewcommand{\arraystretch}{1.25}
%   \setlength{\tabcolsep}{6pt}
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{P{1.8cm} P{3.0cm} P{4.5cm} P{4.8cm}}
%       \hline
%       \textbf{Category} & \textbf{Models / Methods} & \textbf{Key Idea / Contribution} & \textbf{Limitations} \\
%       \hline
%       
%       \textbf{DETR} 
%       & UP-DETR, Group DETR v2, Plain-DETR, Co-DETR, RT-DETR, LW-DETR, Salience DETR
%       & Unsupervised and encoder--decoder pretraining; MIM-based backbones; hybrid assignments; real-time DETR; salience-guided refinement
%       & High computational cost; slow convergence; weak performance on small lesions; limited domain transfer to MRI \\
%       
%       \hline
%       
%       \textbf{Brain Tumor Detection} 
%       & RCS-YOLO, BGF-YOLO, YOLOv5, YOLOv8, YOLOv9
%       & Region concentration modules; multiscale attentional fusion; fast and accurate object detection; real-time capability
%       & Lacks domain-specific pretraining; struggles with small tumors; poor cross-plane generalization across axial/coronal/sagittal MRI \\
%       
%       \hline
%       
%       \textbf{Multiplanar MRI Analysis} 
%       & 2D CNN ensembles (Piantadosi et al.); MPS-FFA; YOLOv5m multiplanar study
%       & Leverages complementary axial/coronal/sagittal views; multiplane + multiscale feature fusion; improved semantic richness
%       & Variation in lesion appearance across planes; more small lesions in thin slices; orientation-dependent differences; single model struggles to perform equally across all planes \\
%       
%       \hline
%     \end{tabular}
%   }
% \end{table*}

\subsection{Mixture-of-Experts}
Integrating various knowledge into one for machine learning is especially important as it can enable utilization of pretrained knowledge.
N.~Shazeer \textit{et al}.~\cite{mixture-of-experts} proposed a neural network architecture called Mixture-of-Experts which considers each model as an expert and uses a gating layer to combine the models into one.
Its potential lies in computational efficiency, scalability, and leveraging specialized expert modules.
G.~Chen \textit{et al}.~\cite{lion} used router module for multimodal large language model (MLLM) called LION, treating each AdaptFormer \cite{adaptformer} as an expert.
Each AdaptFormer is allocated to perform image-level and region-level task given the input image and prompt, and LION adopts a router module to control the ratio between image-level and region-level knowledge.
In terms of medical machine learning area, some works \cite{li2021medical, yadav2025scalable} employ mixture-of-experts architecture to handle brain tumor detection problem.


\section{Method}

In this section, we present the Mixture-PK-YOLO model.
The proposed model aims to utilize 3D MRI slice knowledge to bridge the gap caused by the domain-shift from the axial plane to other planes.
As discussed in Section.~\ref{sec:limits}, one of the most notable limitations of the original PK-YOLO stems from its performance drop when sagittal plane images are given to the model compared to axial plane images.
Inspired by the mixture-of-experts models, Mixture-PK-YOLO employs three backbone models, compared to the PK-YOLO with one single backbone model, each is treated as an expert to its corresponding plane data.
This approach is to capture each plane-specific visual cues for each backbone models.

Since the proposed model have added additional two more backbone models, it is necessary to combine the outputs from the backbone models into one.
As shown in Figure~\ref{fig:model_comp}, router module is adopted to combine the outputs from the backbone models, then feed the SparK RepViT's output to the auxiliary CBNet and YOLOv9 as PK-YOLO did.

% TODO: describe more about the model's input and output
Assuming $X \in \mathcal{R}^{3\times D \times D}$ is the input image given to the model,

\begin{align}
  Z = \sum_{k=1}^{3}R_k(X) \cdot W_k,
\end{align}

\noindent where $R_k(X)$ is a $k$-th plane-specific backbone applied to the input image $X$,
and $W_k$ represents $k$-th plane-specific trainable weight vector controlling the importance of its plane information.

\begin{figure*}
  \centerline{\includegraphics[width=\textwidth]{images/model_comp.png}}
  \caption{Comparison between the original PK-YOLO and the proposed PK-YOLO. Original PK-YOLO model uses single pretrained backbone model, namely SparK RepViT, fed with axial plane data. Mixture PK-YOLO employs three backbones treating each backbone as an expert to its corresponding plane data. Then a router module integrates the outputs from the backbones into one, then pass it to YOLO model.}
  \label{fig:model_comp}
\end{figure*}

\section{Experiments}

\section{Conclusion}


\bibliographystyle{abbrv}
\bibliography{bibfile}

\end{document}
